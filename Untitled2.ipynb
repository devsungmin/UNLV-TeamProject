{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import pkg_resources\n",
    "import shutil\n",
    "import logging\n",
    "import time\n",
    "import subprocess\n",
    "import copy\n",
    "import diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fasta 파일형식으로 재출력\n",
    "def preprocessing(seq_num,temp_file,seq_id,seq_ec ):\n",
    "    data_size=len(seq_num) \n",
    "    seq_n=np.array(seq_num) \n",
    "    seq_i=np.array(seq_id) \n",
    "    seq_e=np.array(seq_ec)\n",
    "    l = 1000 \n",
    "    fp = open(temp_file, 'w')\n",
    "\n",
    "    #input_handle = open(fasta_file, \"r\")\n",
    "\n",
    "    for i in range (data_size) :\n",
    "        try:\n",
    "            seq_id = \"\".join(map(str,seq_i[i]))\n",
    "            seq = \"\".join(map(str,seq_n[i]))\n",
    "            seq_ec = \"\".join(map(str,seq_e[i]))\n",
    "\n",
    "            if len(seq) <= 1000:\n",
    "                fp.write('>%s\\n'%(seq_id))\n",
    "                fp.write('%s\\n'%(seq.strip()))\n",
    "            else:\n",
    "                for i in range(0, len(seq)-l+1, 100):\n",
    "                    new_seq_id = '%s_SEPARATED_SEQUENCE_(%s_%s)' % (seq_id, i+1, i+l+1)\n",
    "                    new_seq = seq[i:i+l]\n",
    "                    fp.write('>%s\\n'%(new_seq_id))\n",
    "                    fp.write('%s\\n'%(new_seq))\n",
    "        except:\n",
    "            pass\n",
    "    #input_handle.close()\n",
    "    fp.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000길이까지 빈 공강 _로 채움\n",
    "def fill_aa(seq):\n",
    "    fill_aa_cnt = 1000 - len(seq)\n",
    "    add_aa_seq = '_' * fill_aa_cnt\n",
    "    new_seq = seq + add_aa_seq\n",
    "    return new_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성이 같으면 1 특성이 다르면 0 one hot encoding\n",
    "def score_info():\n",
    "    aa_list = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X', '_']\n",
    "    aa_score_info = {}\n",
    "    for aa in aa_list:\n",
    "        for aa2 in aa_list:\n",
    "            if aa == aa2:\n",
    "                aa_score_info[(aa, aa2)] = 1.0\n",
    "                aa_score_info[(aa2, aa)] = 1.0\n",
    "            else:\n",
    "                aa_score_info[(aa, aa2)] = 0.0\n",
    "                aa_score_info[(aa2, aa)] = 0.0\n",
    "    return aa_score_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(seq, aa_score_info):\n",
    "    data = np.zeros((1000, 21), dtype=np.float32)\n",
    "    aa_list = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y', 'X']\n",
    "    for i, aa in enumerate(seq):\n",
    "        for j, aa2 in enumerate(aa_list):\n",
    "            data[i, j] = aa_score_info[aa, aa2]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n자임 예측\n",
    "def predict_target_ec(df, output_file, DeepEC_model):\n",
    "    seq_ids = list(df.index)\n",
    "    X_temp = df.values\n",
    "    new_X = []\n",
    "    for i in range(len(X_temp)):\n",
    "        temp = np.reshape(X_temp[i], (1000, 21))\n",
    "        new_X.append(temp)\n",
    "    \n",
    "    X = np.asarray(new_X)\n",
    "    X = X.reshape(X.shape[0], 1000, 21, 1)\n",
    "    \n",
    "    model = load_model(DeepEC_model)\n",
    "    \n",
    "    y_predicted = model.predict(X)\n",
    "    enzyme_list = []\n",
    "    with open(output_file, 'w') as fp:\n",
    "        fp.write('Query ID\\tPredicted class\\tDNN activity\\n')\n",
    "        for i in range(len(y_predicted)): \n",
    "            socre = y_predicted[i][1]\n",
    "            if y_predicted[i][1] > 0.5:\n",
    "                enzyme_list.append(seq_ids[i])\n",
    "                fp.write('%s\\t%s\\t%s\\n'%(seq_ids[i], 'Target', socre))\n",
    "            else:\n",
    "                fp.write('%s\\t%s\\t%s\\n'%(seq_ids[i], 'Non-target', 1-socre))\n",
    "    return enzyme_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_enzyme(df, output_file, DeepEC_model):\n",
    "    seq_ids = list(df.index)\n",
    "    X_temp = df.values\n",
    "    new_X = []\n",
    "    for i in range(len(X_temp)):\n",
    "        temp = np.reshape(X_temp[i], (1000, 21))\n",
    "        new_X.append(temp)\n",
    "    \n",
    "    X = np.asarray(new_X)\n",
    "    X = X.reshape(X.shape[0], 1000, 21, 1)\n",
    "    \n",
    "    #model = load_model(DeepEC_model)\n",
    "    model = DeepEC_model\n",
    "    \n",
    "    y_predicted = model.predict(X)\n",
    "    enzyme_list = []\n",
    "    with open(output_file, 'w') as fp:\n",
    "        fp.write('Query ID\\tPredicted class\\tDNN activity\\n')\n",
    "        for i in range(len(y_predicted)): \n",
    "            socre = y_predicted[i][1]\n",
    "            if y_predicted[i][1] > 0.5:\n",
    "                enzyme_list.append(seq_ids[i])\n",
    "                fp.write('%s\\t%s\\t%s\\n'%(seq_ids[i], 'Enzyme', socre))\n",
    "            else:\n",
    "                fp.write('%s\\t%s\\t%s\\n'%(seq_ids[i], 'Non-enzyme', 1-socre))\n",
    "    return enzyme_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ec(df, output_file, DeepEC_model, MultiLabelBinarizer, threshold):\n",
    "    if (sys.version_info > (3, 0)):\n",
    "        import _pickle as cPickle\n",
    "    else:\n",
    "        import cPickle\n",
    "    \n",
    "    seq_ids = list(df.index)\n",
    "    X_temp = df.values\n",
    "    new_X = []\n",
    "    for i in range(len(X_temp)):\n",
    "        temp = np.reshape(X_temp[i], (1000, 21))\n",
    "        new_X.append(temp)\n",
    "    \n",
    "    X = np.asarray(new_X)\n",
    "    X = X.reshape(X.shape[0], 1000, 21, 1)\n",
    "    \n",
    "    with open(MultiLabelBinarizer, 'rb') as fid:\n",
    "        lb = cPickle.load(fid)\n",
    "    \n",
    "    model = DeepEC_model\n",
    "    \n",
    "    y_predicted = model.predict(X)\n",
    "    original_y_predicted = copy.deepcopy(y_predicted)\n",
    "    y_predicted[y_predicted >= threshold] = 1\n",
    "    y_predicted[y_predicted <threshold] = 0\n",
    "    y_predicted_results = lb.inverse_transform(y_predicted) \n",
    "\n",
    "    with open(output_file, 'w') as fp:\n",
    "        fp.write('Query ID\\tPredicted EC number\\tDNN activity\\n')\n",
    "        for i in range(len(y_predicted)): \n",
    "            each_y_predicted = copy.deepcopy(y_predicted[i])\n",
    "            predicted_ddi_score = original_y_predicted[i]\n",
    "            target_index = np.flatnonzero(each_y_predicted == 1)\n",
    "            if len(target_index) > 0:\n",
    "                for each_idx in target_index:\n",
    "                    each_y_predicted = copy.deepcopy(y_predicted[i])\n",
    "                    each_y_predicted[:] = 0\n",
    "                    each_y_predicted[each_idx] = 1\n",
    "\n",
    "                    each_y_predicted = np.array([list(each_y_predicted)])\n",
    "                    y_transformed = lb.inverse_transform(each_y_predicted) \n",
    "\n",
    "                    score = predicted_ddi_score[each_idx]\n",
    "                    ec_number = y_transformed[0][0]\n",
    "                    fp.write('%s\\t%s\\t%s\\n'%(seq_ids[i], ec_number, score))\n",
    "            \n",
    "            else:\n",
    "                fp.write('%s\\t%s\\t%s\\n'%( seq_ids[i], 'EC number not predicted', 'N/A'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_hot_encoding1(fasta_file,temp_file,seq_ec): #prcessing 하고 돌릴 경우\n",
    "    input_handle = open(fasta_file, \"r\")\n",
    "    \n",
    "    data_size=len(seq_num)\n",
    "    seq_n=np.array(seq_num)\n",
    "    seq_i=np.array(seq_id)\n",
    "    seq_e=np.array(seq_ec)\n",
    "    aa_score_info = score_info()\n",
    "    fp = open(temp_file, 'w')\n",
    "    feature_names = ['Feature%s'%(i) for i in range(1, 21001)]     \n",
    "    fp.write('%s\\n'%(','.join(['ID']+['EC']+feature_names)))\n",
    "\n",
    "    for i in range (data_size) :\n",
    "        try:\n",
    "            seq_id = \"\".join(map(str,seq_i[i]))\n",
    "            seq = \"\".join(map(str,seq_n[i]))\n",
    "            seq_ec = \"\".join(map(str,seq_e[i]))\n",
    "\n",
    "            \n",
    "            if len(seq) >= 10 and len(seq) <= 1000:\n",
    "                if len(seq) < 1000:\n",
    "                    seq = fill_aa(seq)\n",
    "                encoded_vector = one_hot_encoding(seq, aa_score_info)\n",
    "                #print(encoded_vector)\n",
    "                flatten_encoded_vector = encoded_vector.flatten()\n",
    "                flatten_encoded_vector_str = [str(each_val) for each_val in flatten_encoded_vector]\n",
    "                fp.write('%s\\n'%(','.join([seq_id]+[seq_ec]+flatten_encoded_vector_str)))\n",
    "        except:\n",
    "            pass\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_one_hot_encoding2(temp_file,seq_num,seq_id,seq_ec): # processing 안하고 돌릴경우\n",
    "    \n",
    "    \n",
    "    data_size=len(seq_num)\n",
    "    seq_n=np.array(seq_num)\n",
    "    seq_i=np.array(seq_id)\n",
    "    seq_e=np.array(seq_ec)\n",
    "    aa_score_info = score_info()\n",
    "    fp = open(temp_file, 'w')\n",
    "    feature_names = ['Feature%s'%(i) for i in range(1, 21001)]     \n",
    "    fp.write('%s\\n'%(','.join(['ID']+['DSEQ']+['EC']+feature_names)))\n",
    "\n",
    "    for i in range (data_size) :\n",
    "        try:\n",
    "            seq_id = \"\".join(map(str,seq_i[i]))\n",
    "            seq = \"\".join(map(str,seq_n[i]))\n",
    "            seq_ec = \"\".join(map(str,seq_e[i]))\n",
    "            dseq = \"\".join(map(str,seq_n[i]))\n",
    "            \n",
    "            if len(seq) >= 10 and len(seq) <= 1000:\n",
    "                if len(seq) < 1000:\n",
    "                    seq = fill_aa(seq)\n",
    "                \n",
    "                encoded_vector = one_hot_encoding(seq, aa_score_info)\n",
    "                #print(encoded_vector)\n",
    "                flatten_encoded_vector = encoded_vector.flatten()\n",
    "                flatten_encoded_vector_str = [str(each_val) for each_val in flatten_encoded_vector]\n",
    "                fp.write('%s\\n'%(','.join([seq_id]+[dseq]+[seq_ec]+flatten_encoded_vector_str)))\n",
    "        except:\n",
    "            pass\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dl_prediction_results(dl4_output_file, dl3_output_file, enzyme_output_file, data_seq, target_fasta_file,data_id):\n",
    "    enzyme_results = {}\n",
    "    with open(enzyme_output_file, 'r') as fp:\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            sptlist = line.strip().split('\\t')\n",
    "            query = sptlist[0].strip()\n",
    "            if '_SEPARATED_SEQUENCE_' in query:\n",
    "                query = query.split('_SEPARATED_SEQUENCE_')[0].strip()\n",
    "                    \n",
    "            enzyme_info = sptlist[1].strip()\n",
    "            if 'Non-enzyme' not in enzyme_info:\n",
    "                enzyme_results[query] = 1\n",
    "\n",
    "    ec_4digit_results = read_prediction_results(dl4_output_file)\n",
    "    ec_3digit_results = read_prediction_results(dl3_output_file)\n",
    "    \n",
    "    final_one_digit_ec_results = get_prediction_results(ec_4digit_results, ec_3digit_results, enzyme_results)\n",
    "    \n",
    "    with open('DeepEC_Result_DL.txt', 'w') as fp:\n",
    "        fp.write('Query ID\tPredicted EC number\\n')\n",
    "        for each_query in final_one_digit_ec_results:\n",
    "            for each_predicted_ec in final_one_digit_ec_results[each_query]:\n",
    "                fp.write('%s\\t%s\\n'%(each_query, each_predicted_ec))\n",
    "                \n",
    "    fp = open(target_fasta_file, 'w')      \n",
    "    #input_handle = open(fasta_file, \"r\")\n",
    "    data_size=len(data_seq)\n",
    "    seq_n=np.array(data_seq) \n",
    "    seq_ids = list(data_seq.index)\n",
    "\n",
    "    for i in range(data_size):\n",
    "        #seq_id = \"\".join(map(str,seq_i[i]))\n",
    "        seq_id=str(seq_ids[i])\n",
    "        seq = \"\".join(map(str,seq_n[i]))\n",
    "\n",
    "        if seq_id in enzyme_results:\n",
    "            if seq_id not in final_one_digit_ec_results:\n",
    "                fp.write('>%s\\n'%(seq_id))\n",
    "                fp.write('%s\\n'%(seq))\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dl_seq_results():\n",
    "    fp2 = open('DeepEC_Result.txt', 'w')\n",
    "    fp2.write('Query ID\tPredicted EC number\\n')\n",
    "    with open('DeepEC_Result_DL.txt', 'r') as fp:\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            fp2.write('%s\\n'%(line.strip()))\n",
    "    try:        \n",
    "        with open('Blastp_result.txt', 'r') as fp:\n",
    "            for line in fp:\n",
    "                fp2.write('%s\\n'%(line.strip()))\n",
    "    except:\n",
    "        pass\n",
    "    fp2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ec_dia(ref_db_file, target_fasta_file, output_file):\n",
    "    uniprot_ec_info = {}\n",
    "    pfam_uniprot_id_info = {}\n",
    "\n",
    "    blastp_result = 'blastp_result_temp.txt'\n",
    "    run_blastp(target_fasta_file, blastp_result, ref_db_file) \n",
    "    seq_based_ec_prediction_results = read_best_blast_result(blastp_result)\n",
    "    with open(output_file, 'w') as fp:\n",
    "        for each_query_id in seq_based_ec_prediction_results:\n",
    "            each_ec_number = seq_based_ec_prediction_results[each_query_id][0]\n",
    "            fp.write('%s\\t%s\\n'%(each_query_id, each_ec_number))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_blastp(target_fasta, blastp_result, db_dir):\n",
    "    threads = 1\n",
    "    subprocess.call(\"diamond blastp -d %s -q %s -o %s --threads %s --id 50 --outfmt 6 qseqid sseqid evalue score qlen slen length pident\"%(db_dir, target_fasta, blastp_result, threads), shell=True, stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_best_blast_result(blastp_result):\n",
    "    query_db_set_info = {}\n",
    "    with open(blastp_result, 'r') as fp:\n",
    "        for line in fp:\n",
    "            sptlist = line.strip().split('\\t')            \n",
    "            query_id = sptlist[0].strip()\n",
    "            db_id = sptlist[1].strip()  \n",
    "            \n",
    "            #ec_number = db_id.split('|')[1].strip()\n",
    "            score = float(sptlist[3].strip())\n",
    "            qlen = sptlist[4].strip()\n",
    "            length = sptlist[6].strip()\n",
    "            length = float(length)\n",
    "            \n",
    "            coverage = length/float(qlen)*100\n",
    "            if coverage >= 75:\n",
    "                if query_id not in query_db_set_info:\n",
    "                    query_db_set_info[query_id] = [ec_number, score]\n",
    "                else:\n",
    "                    p_score = query_db_set_info[query_id][1]\n",
    "                    if score > p_score:\n",
    "                        query_db_set_info[query_id] = [ec_number, score]\n",
    "    return query_db_set_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ec_list(filename):\n",
    "    ec_list = []\n",
    "    with open(filename, 'r') as fp:\n",
    "        for line in fp:\n",
    "            ec_list.append(line.strip())\n",
    "    return ec_list\n",
    "\n",
    "def read_prediction_results(result_file):\n",
    "    ec_results = {}\n",
    "    with open(result_file, 'r') as fp:\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            sptlist = line.strip().split('\\t')\n",
    "            query = sptlist[0].strip()\n",
    "            if '_SEPARATED_SEQUENCE_' in query:\n",
    "                query = query.split('_SEPARATED_SEQUENCE_')[0].strip()\n",
    "            predicted_ec = sptlist[1].strip()\n",
    "            dnn_activity = sptlist[2].strip()\n",
    "            if predicted_ec != 'EC number not predicted':\n",
    "                if query not in ec_results:\n",
    "                    ec_results[query] = [predicted_ec]\n",
    "                else:\n",
    "                    ec_results[query].append(predicted_ec)\n",
    "                    ec_results[query] = list(set(ec_results[query]))\n",
    "    #print(ec_results)\n",
    "    return ec_results\n",
    "\n",
    "def get_prediction_results(ec_4digit_results, ec_3digit_results, enzyme_results):\n",
    "    final_one_digit_ec_results = {}\n",
    "    for query in ec_4digit_results:\n",
    "        if query in enzyme_results:\n",
    "            predicted_ec_list = ec_4digit_results[query]\n",
    "            for predicted_ec in predicted_ec_list:\n",
    "                flag3 = False\n",
    "                if query in ec_3digit_results:\n",
    "                    for each_digit_ec in ec_3digit_results[query]:\n",
    "                        if each_digit_ec in predicted_ec:\n",
    "                            flag3 = True\n",
    "                        #else:\n",
    "                            #print(each_digit_ec)\n",
    "                            \n",
    "                \n",
    "                if flag3 == True:\n",
    "                    if query not in final_one_digit_ec_results:\n",
    "                        final_one_digit_ec_results[query] = [predicted_ec]\n",
    "                        \n",
    "                    else:\n",
    "                        final_one_digit_ec_results[query].append(predicted_ec)\n",
    "                        #print(predicted_ec)\n",
    "                        final_one_digit_ec_results[query] = list(set(final_one_digit_ec_results[query]))\n",
    "                        \n",
    "                        \n",
    "    return final_one_digit_ec_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_1():\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate, BatchNormalization, Activation\n",
    "    from keras.models import Model\n",
    "    # 입력층을 정의\n",
    "    input_1 = Input(shape=(1000,21,1))\n",
    "    \n",
    "    # 첫번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    x = Conv2D(128, (4, 21), activation='relu')(input_1)\n",
    "    x = MaxPooling2D((997, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "   \n",
    "\n",
    "    # 두번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    y = Conv2D(128, (8, 21), activation='relu')(input_1)\n",
    "    y = MaxPooling2D((993, 1))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = BatchNormalization()(y)\n",
    "   \n",
    "    \n",
    "    # 세번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    z = Conv2D(128, (16, 21), activation='relu')(input_1)\n",
    "    z = MaxPooling2D((985, 1))(z)\n",
    "    z = Flatten()(z)\n",
    "    z = BatchNormalization()(z)\n",
    " \n",
    "    \n",
    "    # 세개의 인공 신경망의 출력을 연결(concatenate)\n",
    "    result = concatenate([x, y, z])\n",
    "\n",
    "    # 연결된 값을 입력으로 받는 밀집층을 추가(Dense layer)\n",
    "    g = Dense(512,kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=0.009999999776482582))(result)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(512,kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=0.009999999776482582))(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(2)(g)\n",
    "    \n",
    "    # 결과적으로 이 모델은 3 개의 입력층으로부터 분기되어 진행된 후 마지막에는 하나의 출력을 예측하는 모델이 됨.\n",
    "    model = Model(inputs=input_1, outputs=g)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_2():\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate, BatchNormalization, Activation\n",
    "    from keras.models import Model\n",
    "    # 입력층을 정의\n",
    "    input_1 = Input(shape=(1000,21,1))\n",
    "    \n",
    "    # 첫번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    x = Conv2D(128, (4, 21), activation='relu')(input_1)\n",
    "    x = MaxPooling2D((997, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "   \n",
    "\n",
    "    # 두번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    y = Conv2D(128, (8, 21), activation='relu')(input_1)\n",
    "    y = MaxPooling2D((993, 1))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = BatchNormalization()(y)\n",
    "   \n",
    "    \n",
    "    # 세번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    z = Conv2D(128, (16, 21), activation='relu')(input_1)\n",
    "    z = MaxPooling2D((985, 1))(z)\n",
    "    z = Flatten()(z)\n",
    "    z = BatchNormalization()(z)\n",
    " \n",
    "    \n",
    "    # 세개의 인공 신경망의 출력을 연결(concatenate)\n",
    "    result = concatenate([x, y, z])\n",
    "\n",
    "    # 연결된 값을 입력으로 받는 밀집층을 추가(Dense layer)\n",
    "    g = Dense(512)(result)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(512)(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(216)(g)\n",
    "    \n",
    "    # 결과적으로 이 모델은 3 개의 입력층으로부터 분기되어 진행된 후 마지막에는 하나의 출력을 예측하는 모델이 됨.\n",
    "    model = Model(inputs=input_1, outputs=g)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_3():\n",
    "    from keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten, concatenate, BatchNormalization, Activation\n",
    "    from keras.models import Model\n",
    "    # 입력층을 정의\n",
    "    input_1 = Input(shape=(1000,21,1))\n",
    "    \n",
    "    # 첫번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    x = Conv2D(128, (4, 21), activation='relu')(input_1)\n",
    "    x = MaxPooling2D((997, 1))(x)\n",
    "    x = Flatten()(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "   \n",
    "\n",
    "    # 두번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    y = Conv2D(128, (8, 21), activation='relu')(input_1)\n",
    "    y = MaxPooling2D((993, 1))(y)\n",
    "    y = Flatten()(y)\n",
    "    y = BatchNormalization()(y)\n",
    "   \n",
    "    \n",
    "    # 세번째 층으로부터 분기되어 진행되는 인공 신경망을 정의\n",
    "    z = Conv2D(128, (16, 21), activation='relu')(input_1)\n",
    "    z = MaxPooling2D((985, 1))(z)\n",
    "    z = Flatten()(z)\n",
    "    z = BatchNormalization()(z)\n",
    " \n",
    "    \n",
    "    # 세개의 인공 신경망의 출력을 연결(concatenate)\n",
    "    result = concatenate([x, y, z])\n",
    "\n",
    "    # 연결된 값을 입력으로 받는 밀집층을 추가(Dense layer)\n",
    "    g = Dense(512,kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=0.009999999776482582))(result)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(512,kernel_regularizer=regularizers.l1_l2(l1=0.00, l2=0.009999999776482582))(g)\n",
    "    g = BatchNormalization()(g)\n",
    "    g = Activation('relu')(g)\n",
    "    g = Dense(2429)(g)\n",
    "    \n",
    "    # 결과적으로 이 모델은 3 개의 입력층으로부터 분기되어 진행된 후 마지막에는 하나의 출력을 예측하는 모델이 됨.\n",
    "    model = Model(inputs=input_1, outputs=g)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.-.-.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.-.-.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.-.-.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.-.-.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.-.-.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10903</th>\n",
       "      <td>7.5.2.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10904</th>\n",
       "      <td>7.5.2.-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10905</th>\n",
       "      <td>7.5.2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10906</th>\n",
       "      <td>7.6.2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10907</th>\n",
       "      <td>7.6.2.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10908 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            EC\n",
       "0      1.-.-.-\n",
       "1      1.-.-.-\n",
       "2      1.-.-.-\n",
       "3      1.-.-.-\n",
       "4      1.-.-.-\n",
       "...        ...\n",
       "10903  7.5.2.-\n",
       "10904  7.5.2.-\n",
       "10905  7.5.2.1\n",
       "10906  7.6.2.8\n",
       "10907  7.6.2.8\n",
       "\n",
       "[10908 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_data = pd.read_csv(\"EC number database_Archaea_ec.csv\")\n",
    "seq_num = seq_data.drop('AA',axis=1)\n",
    "seq_num = seq_num.drop('AC',axis=1)\n",
    "seq_num = seq_num.drop('DE',axis=1)\n",
    "#seq_num = seq_num.drop('EC',axis=1)\n",
    "seq_num = seq_num.drop('MW',axis=1)\n",
    "seq_num = seq_num.drop('OC',axis=1)\n",
    "seq_num = seq_num.drop('OS',axis=1)\n",
    "#seq_num = seq_num.drop('Unnamed: 0',axis=1)\n",
    "seq_ec= seq_num.drop('SEQ',axis=1)\n",
    "#seq_id= seq_num.drop('SEQ',axis=1)\n",
    "#seq_num= seq_num.drop('ID',axis=1)\n",
    "seq_ec=seq_num.drop('SEQ',axis=1)\n",
    "seq_num= seq_num.drop('EC',axis=1)\n",
    "#seq_id = seq_id.drop('EC',axis=1)\n",
    "seq_ec=seq_ec.drop('ID',axis=1)\n",
    "seq_id=seq_num.drop('SEQ',axis=1)\n",
    "seq_num=seq_num.drop('ID',axis=1)\n",
    "seq_e=np.array(seq_ec)\n",
    "fp = open(\"EC_LABEL\", 'w') \n",
    "fp.write('%s\\n'%(['EC']))\n",
    "for i in range(len(seq_ec)):\n",
    "    seq_ecs = \"\".join(map(str,seq_e[i]))\n",
    "    fp.write('%s\\n'%(seq_ecs))\n",
    "fp.close()\n",
    "seq_ec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing(seq_num,\"fasta_temp_file\",seq_id,seq_ec)\n",
    "#\"fasta_temp_file\"\n",
    "temp_file=\"temp_file1.csv\"\n",
    "#run_one_hot_encoding1(\"fasta_temp_file\",temp_file,seq_ec)\n",
    "#run_one_hot_encoding2(temp_file,seq_num,seq_id,seq_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10727, 21003)\n"
     ]
    }
   ],
   "source": [
    "encode_data = pd.read_csv(temp_file)\n",
    "print(encode_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature1</th>\n",
       "      <th>Feature2</th>\n",
       "      <th>Feature3</th>\n",
       "      <th>Feature4</th>\n",
       "      <th>Feature5</th>\n",
       "      <th>Feature6</th>\n",
       "      <th>Feature7</th>\n",
       "      <th>Feature8</th>\n",
       "      <th>Feature9</th>\n",
       "      <th>Feature10</th>\n",
       "      <th>...</th>\n",
       "      <th>Feature20991</th>\n",
       "      <th>Feature20992</th>\n",
       "      <th>Feature20993</th>\n",
       "      <th>Feature20994</th>\n",
       "      <th>Feature20995</th>\n",
       "      <th>Feature20996</th>\n",
       "      <th>Feature20997</th>\n",
       "      <th>Feature20998</th>\n",
       "      <th>Feature20999</th>\n",
       "      <th>Feature21000</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4352</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5081</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10547</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2734</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7739</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10311</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4017</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7508 rows × 21000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature1  Feature2  Feature3  Feature4  Feature5  Feature6  Feature7  \\\n",
       "4352        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "5081        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "10547       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2734        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "7739        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "5789        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "10311       0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "4017        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "7767        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2982        0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "       Feature8  Feature9  Feature10  ...  Feature20991  Feature20992  \\\n",
       "4352        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "5081        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "10547       0.0       0.0        0.0  ...           0.0           0.0   \n",
       "2734        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "7739        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "...         ...       ...        ...  ...           ...           ...   \n",
       "5789        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "10311       0.0       0.0        0.0  ...           0.0           0.0   \n",
       "4017        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "7767        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "2982        0.0       0.0        0.0  ...           0.0           0.0   \n",
       "\n",
       "       Feature20993  Feature20994  Feature20995  Feature20996  Feature20997  \\\n",
       "4352            0.0           0.0           0.0           0.0           0.0   \n",
       "5081            0.0           0.0           0.0           0.0           0.0   \n",
       "10547           0.0           0.0           0.0           0.0           0.0   \n",
       "2734            0.0           0.0           0.0           0.0           0.0   \n",
       "7739            0.0           0.0           0.0           0.0           0.0   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "5789            0.0           0.0           0.0           0.0           0.0   \n",
       "10311           0.0           0.0           0.0           0.0           0.0   \n",
       "4017            0.0           0.0           0.0           0.0           0.0   \n",
       "7767            0.0           0.0           0.0           0.0           0.0   \n",
       "2982            0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "       Feature20998  Feature20999  Feature21000  \n",
       "4352            0.0           0.0           0.0  \n",
       "5081            0.0           0.0           0.0  \n",
       "10547           0.0           0.0           0.0  \n",
       "2734            0.0           0.0           0.0  \n",
       "7739            0.0           0.0           0.0  \n",
       "...             ...           ...           ...  \n",
       "5789            0.0           0.0           0.0  \n",
       "10311           0.0           0.0           0.0  \n",
       "4017            0.0           0.0           0.0  \n",
       "7767            0.0           0.0           0.0  \n",
       "2982            0.0           0.0           0.0  \n",
       "\n",
       "[7508 rows x 21000 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data= encode_data\n",
    "#x_data = encode_data.drop('ID',axis=1)\n",
    "#x_data = x_data.drop('EC',axis=1)\n",
    "#x_data = x_data.drop('DESQ',axis=1)\n",
    "y_data = encode_data[\"EC\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777)\n",
    "data_id = X_train[\"ID\"]\n",
    "data_seq = X_train[\"DSEQ\"]\n",
    "X_train= X_train.drop('ID',axis=1)\n",
    "X_train= X_train.drop('DSEQ',axis=1)\n",
    "X_train= X_train.drop('EC',axis=1)\n",
    "\n",
    "y_train\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1= keras.models.load_model(r'Binary_class.h5')\n",
    "#model2= keras.models.load_model(r'DeepEC_3d.h5')\n",
    "#model3= keras.models.load_model(r'DeepEC_4d.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=cnn_model_1()\n",
    "model2=cnn_model_2()\n",
    "model3=cnn_model_3()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enzyme_output_file = 'Enzyme_prediction.txt'\n",
    "predict_enzyme(X_train, enzyme_output_file, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dl3_output_file = '3digit_EC_prediction.txt'\n",
    "predict_ec(X_train,dl3_output_file, model2, 'multilabelbinarizer_dl_3digit.pkl', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl4_output_file = '4digit_EC_prediction.txt'\n",
    "predict_ec(X_train,dl4_output_file, model3, 'multilabelbinarizer_dl_4digit.pkl', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ec_number' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-4a028743de88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mref_db_file\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'database_total.fasta'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Blastp_result.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredict_ec_dia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mref_db_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_fasta_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmerge_dl_seq_results\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-df72b080f903>\u001b[0m in \u001b[0;36mpredict_ec_dia\u001b[1;34m(ref_db_file, target_fasta_file, output_file)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mblastp_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'blastp_result_temp.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mrun_blastp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_fasta_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblastp_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_db_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mseq_based_ec_prediction_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_best_blast_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblastp_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0meach_query_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseq_based_ec_prediction_results\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-79b0ef4b2f8d>\u001b[0m in \u001b[0;36mread_best_blast_result\u001b[1;34m(blastp_result)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcoverage\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mquery_id\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery_db_set_info\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m                     \u001b[0mquery_db_set_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mec_number\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m                     \u001b[0mp_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquery_db_set_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ec_number' is not defined"
     ]
    }
   ],
   "source": [
    "target_fasta_file = 'low_seq_candidates.txt'\n",
    "merge_dl_prediction_results(dl4_output_file, dl3_output_file, enzyme_output_file, data_seq, target_fasta_file,data_id) \n",
    "ref_db_file ='database_total.fasta'\n",
    "output_file = 'Blastp_result.txt'\n",
    "predict_ec_dia(ref_db_file, target_fasta_file, output_file)\n",
    "\n",
    "merge_dl_seq_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hi(seq_num,seq_ec,seq_id):\n",
    "    #fp = open(\"EC number database_Archaea_ec.csv\",'r')\n",
    "    fp2 = open(\"database_total.fasta\",'w')\n",
    "\n",
    "    data_size=len(seq_num)\n",
    "    seq_n=np.array(seq_num) \n",
    "    seq_i=np.array(seq_id) \n",
    "    seq_e=np.array(seq_ec)\n",
    "\n",
    "    for i in range (data_size):\n",
    "        seq_id = \"\".join(map(str,seq_i[i]))\n",
    "        seq = \"\".join(map(str,seq_n[i]))\n",
    "        seq_ec = \"\".join(map(str,seq_e[i]))\n",
    "        fp2.write('>%s|%s\\n'%(seq_id,seq_ec))\n",
    "        fp2.write('%s\\n'%(seq.strip()))\n",
    "    #fp.close\n",
    "    fp2.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hi(seq_num,seq_ec,seq_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1000, 21, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 997, 1, 128)  10880       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 993, 1, 128)  21632       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 985, 1, 128)  43136       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 128)          512         flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           batch_normalization_1[0][0]      \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          197120      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 512)          2048        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 512)          2048        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 512)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            1026        activation_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 542,082\n",
      "Trainable params: 539,266\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 1000, 21, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 997, 1, 128)  10880       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 993, 1, 128)  21632       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 985, 1, 128)  43136       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 128)          512         flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 128)          512         flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 128)          512         flatten_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 384)          0           batch_normalization_6[0][0]      \n",
      "                                                                 batch_normalization_7[0][0]      \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          197120      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 512)          2048        dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 512)          0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512)          262656      activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 512)          2048        dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 512)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 216)          110808      activation_4[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 651,864\n",
      "Trainable params: 649,048\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1000, 21, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 997, 1, 128)  10880       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 993, 1, 128)  21632       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 985, 1, 128)  43136       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 128)          0           max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)             (None, 128)          0           max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 128)          0           max_pooling2d_9[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 128)          512         flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 128)          512         flatten_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128)          512         flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 384)          0           batch_normalization_11[0][0]     \n",
      "                                                                 batch_normalization_12[0][0]     \n",
      "                                                                 batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 512)          197120      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 512)          2048        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 512)          0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 512)          262656      activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 512)          2048        dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 512)          0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 2429)         1246077     activation_6[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,787,133\n",
      "Trainable params: 1,784,317\n",
      "Non-trainable params: 2,816\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary() # binary\n",
    "model2.summary() # 3d\n",
    "model3.summary() # 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
